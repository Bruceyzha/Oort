#!/bin/bash
#
#BSUB -W 24:00                          # 6 hours of walltime requested
#BSUB -n 15                            # number of tasks in job; we get (4 nodes x 40 CPU slots), one gpu node has 40 cpu slots
#BSUB -R "span[ptile=15]"               # limit 40 processes per node. See note above about HT
#BSUB -q gpu_p100                       # choose the queue to use: normal or large_memory
#BSUB -u fanlai@umich.edu                 # email address to send notifications

#rm /tmp/torch/*
source /gpfs/gpfs0/groups/chowdhury/jcgu/script/pytorch_install.sh
source /gpfs/gpfs0/groups/chowdhury/jcgu/mxib/torch1129/bin/activate
