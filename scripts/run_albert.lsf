#!/bin/bash
#
#BSUB -W 1:00                          # 6 hours of walltime requested
#BSUB -n 25                            # number of tasks in job; we get (4 nodes x 40 CPU slots), one gpu node has 40 cpu slots
#BSUB -R "span[ptile=25]"               # limit 40 processes per node. See note above about HT
#BSUB -q gpu_p100                      # choose the queue to use: normal or large_memory
#BSUB -m "gpu-cn002"
#BSUB -u zxfeng@umich.edu                 # email address to send notifications
#rm /tmp/torch/*
#BSUB -J test
#BSUB -e test.e
#BSUB -o test.o
source /gpfs/gpfs0/groups/chowdhury/zxfeng/pytorch_install.sh
source activate mytorch
#source /gpfs/gpfs0/groups/chowdhury/zxfeng/.conda/envs/mytorch/bin/activate
export TRAIN_FILE=/gpfs/gpfs0/groups/chowdhury/zxfeng/transformers/wikitext-2-raw/wiki.train.raw
export TEST_FILE=/gpfs/gpfs0/groups/chowdhury/zxfeng/transformers/wikitext-2-raw/wiki.test.raw
CUDA_VISIBLE_DEVICES=2 python /gpfs/gpfs0/groups/chowdhury/zxfeng/transformers/examples/run_language_modeling.py \
    --output_dir=output \
    --model_type=albert \
    --model_name_or_path=albert-base-v2 \
    --do_train \
    --train_data_file=$TRAIN_FILE \
    --do_eval \
    --eval_data_file=$TEST_FILE \
    --mlm \
    --overwrite_output_dir \
    --num_train_epochs=2 

