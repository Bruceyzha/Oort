#!/bin/bash
#
#BSUB -W 6:00                          # 6 hours of walltime requested
#BSUB -n 40                            # number of tasks in job; we get (4 nodes x 40 CPU slots), one gpu node has 40 cpu slots
#BSUB -R "span[ptile=40]"               # limit 40 processes per node. See note above about HT
#BSUB -q gpu_p100                       # choose the queue to use: normal or large_memory
#BSUB -u fanlai@umich.edu                 # email address to send notifications

#BSUB -J learner3
#BSUB -e learner3.e
#BSUB -o learner3.o

source activate pytorch3 
python ~/DMFL/learner.py --ps_ip=10.255.11.91 --model=alexnet --epochs=20000 --upload_epoch=29  --dump_epoch=200 --learning_rate=0.2 --decay_epoch=10 --model_avg=True --total_worker=27 --resampling_interval=1 --batch_size=64 --data_dir=~/cifar10/ --backend=tcp  --learners=1-2-3-4-5  --this_rank=3